{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.utils.prune as prune\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#backbone_model = resnet50_taskonomy(pretrained=True)\n",
    "#model = nn.Sequential(backbone_model,\n",
    "#                     nn.Linear(2048, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((224, 224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "testsize = len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def pruning_scheme_filters(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            #prunes along dimension 0 according to the l2 norm\n",
    "            #this basically prunes amount% of filters\n",
    "            prune.ln_structured(module, name=\"weight\", amount=0.4, n=2, dim=0)\n",
    "    return model\n",
    "\n",
    "def pruning_scheme_random(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.random_unstructured(module, name=\"weight\", amount=0.4)\n",
    "    return model\n",
    "\n",
    "def train_model(model, loader, num_epochs=10, include_pruning=False, scheme=None, oneshot=False):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    if include_pruning == True and oneshot == True:\n",
    "        if scheme == 'random':\n",
    "            print('Setting all masks before training with {} scheme'.format(scheme))\n",
    "            model = pruning_scheme_filters(model)\n",
    "        elif scheme == 'structured':\n",
    "            print('Setting all masks before training with {} scheme'.format(scheme))\n",
    "            model = pruning_scheme_random(model)\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        if include_pruning == True and oneshot == False:\n",
    "            if scheme == 'structured':\n",
    "                print('Setting masks in epoch {} before training with {} scheme'.format(epoch, scheme))\n",
    "\n",
    "                layers = [name for name, module in model.named_modules() if isinstance(module, torch.nn.Conv2d)]\n",
    "                for name, module in model.named_modules():\n",
    "                    if isinstance(module, torch.nn.Conv2d):\n",
    "                        #if name == layers[epoch*2] or name == layers[epoch*2+1]:\n",
    "                        if name == layers[epoch]:\n",
    "                            prune.ln_structured(module, name=\"weight\", amount=0.4, n=2, dim=0)\n",
    "           \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model = model.to(device)\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "    total_loss = running_loss / testsize\n",
    "    total_acc = running_corrects.double() / testsize\n",
    "\n",
    "    print('Loss: {:.4f} Acc: {:.4f}'.format(total_loss, total_acc))\n",
    "    \n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), 'model_wts.pt')\n",
    "    \n",
    "def load_model(model):\n",
    "    model.load_state_dict(torch.load('model_wts.pt'))\n",
    "    return model\n",
    "\n",
    "#count number of zeroed params\n",
    "def how_sparse(model):\n",
    "    total_sparsity = 0\n",
    "    total_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            total_sparsity += float(torch.sum(module.weight == 0))\n",
    "            total_params += float(module.weight.nelement())\n",
    "    return total_sparsity/total_params\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_fresh():\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "    n_inputs = model.fc.in_features\n",
    "\n",
    "    # add more layers as required\n",
    "    classifier = nn.Sequential(nn.Linear(n_inputs, 10))\n",
    "    model.fc = classifier\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model metrics without pruning: </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.933\n",
      "[2,   200] loss: 0.236\n",
      "[3,   200] loss: 0.156\n",
      "[4,   200] loss: 0.109\n",
      "[5,   200] loss: 0.078\n",
      "[6,   200] loss: 0.053\n",
      "[7,   200] loss: 0.039\n",
      "[8,   200] loss: 0.028\n",
      "[9,   200] loss: 0.020\n",
      "[10,   200] loss: 0.015\n",
      "Loss: 0.1918 Acc: 0.9406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model_fresh()\n",
    "train_model(model, trainloader, num_epochs=10, include_pruning=False)\n",
    "evaluate_model(model, testloader)\n",
    "how_sparse(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.6120 Acc: 0.4962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.9257 Acc: 0.3538\n"
     ]
    }
   ],
   "source": [
    "#no training involved - directly pruning fine-tuned model\n",
    "\n",
    "#random\n",
    "model = load_model(get_model_fresh())\n",
    "for name, module in model.named_modules():\n",
    "    # prune 20% of connections in all 2D-conv layers\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.random_unstructured(module, name=\"weight\", amount=0.4)\n",
    "evaluate_model(model, testloader)\n",
    "\n",
    "#structured - by filter\n",
    "model = load_model(get_model_fresh())\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.ln_structured(module, name=\"weight\", amount=0.4, n=2, dim=0)\n",
    "evaluate_model(model, testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Pruning experiments</b> - All experiments take a fine-tuned model and build on top of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting all masks before training with random scheme\n",
      "[1,   200] loss: 0.639\n",
      "[2,   200] loss: 0.251\n",
      "[3,   200] loss: 0.164\n",
      "[4,   200] loss: 0.109\n",
      "[5,   200] loss: 0.069\n",
      "[6,   200] loss: 0.046\n",
      "[7,   200] loss: 0.032\n",
      "[8,   200] loss: 0.021\n",
      "[9,   200] loss: 0.015\n",
      "[10,   200] loss: 0.012\n",
      "Loss: 0.3316 Acc: 0.9085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4000144355037453"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(get_model_fresh())\n",
    "\n",
    "train_model(model, trainloader, num_epochs=10, include_pruning=True, oneshot=True, scheme='random')    \n",
    "evaluate_model(model, testloader)\n",
    "how_sparse(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting all masks before training with structured scheme\n",
      "[1,   200] loss: 0.532\n",
      "[2,   200] loss: 0.171\n",
      "[3,   200] loss: 0.090\n",
      "[4,   200] loss: 0.048\n",
      "[5,   200] loss: 0.026\n",
      "[6,   200] loss: 0.017\n",
      "[7,   200] loss: 0.012\n",
      "[8,   200] loss: 0.009\n",
      "[9,   200] loss: 0.007\n",
      "[10,   200] loss: 0.006\n",
      "Loss: 0.2754 Acc: 0.9244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4000000179100543"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(get_model_fresh())\n",
    "\n",
    "train_model(model, trainloader, num_epochs=10, include_pruning=True, oneshot=True, scheme='structured')    \n",
    "evaluate_model(model, testloader)\n",
    "how_sparse(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting masks in epoch 0 before training with structured scheme\n",
      "[1,   200] loss: 0.053\n",
      "Setting masks in epoch 1 before training with structured scheme\n",
      "[2,   200] loss: 0.044\n",
      "Setting masks in epoch 2 before training with structured scheme\n",
      "[3,   200] loss: 0.047\n",
      "Setting masks in epoch 3 before training with structured scheme\n",
      "[4,   200] loss: 0.034\n",
      "Setting masks in epoch 4 before training with structured scheme\n",
      "[5,   200] loss: 0.038\n",
      "Setting masks in epoch 5 before training with structured scheme\n",
      "[6,   200] loss: 0.081\n",
      "Setting masks in epoch 6 before training with structured scheme\n",
      "[7,   200] loss: 0.049\n",
      "Setting masks in epoch 7 before training with structured scheme\n",
      "[8,   200] loss: 0.023\n",
      "Setting masks in epoch 8 before training with structured scheme\n",
      "[9,   200] loss: 0.054\n",
      "Setting masks in epoch 9 before training with structured scheme\n",
      "[10,   200] loss: 0.033\n",
      "Setting masks in epoch 10 before training with structured scheme\n",
      "[11,   200] loss: 0.083\n",
      "Setting masks in epoch 11 before training with structured scheme\n",
      "[12,   200] loss: 0.116\n",
      "Setting masks in epoch 12 before training with structured scheme\n",
      "[13,   200] loss: 0.037\n",
      "Setting masks in epoch 13 before training with structured scheme\n",
      "[14,   200] loss: 0.066\n",
      "Setting masks in epoch 14 before training with structured scheme\n",
      "[15,   200] loss: 0.045\n",
      "Setting masks in epoch 15 before training with structured scheme\n",
      "[16,   200] loss: 0.070\n",
      "Setting masks in epoch 16 before training with structured scheme\n",
      "[17,   200] loss: 0.062\n",
      "Setting masks in epoch 17 before training with structured scheme\n",
      "[18,   200] loss: 0.034\n",
      "Setting masks in epoch 18 before training with structured scheme\n",
      "[19,   200] loss: 0.041\n",
      "Setting masks in epoch 19 before training with structured scheme\n",
      "[20,   200] loss: 0.044\n",
      "Loss: 0.2718 Acc: 0.9148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4000144355037453"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original model with 93.78% accuracy\n",
    "model = load_model(get_model_fresh())\n",
    "\n",
    "#layer by layer pruning - over epoch some set of filters are removed - from lowest to topmost layers\n",
    "train_model(model, trainloader, num_epochs=20, include_pruning=True, oneshot=False, scheme='structured')    \n",
    "evaluate_model(model, testloader)\n",
    "how_sparse(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
